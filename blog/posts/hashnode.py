pocket_server = "Hi folks! Have you ever thought about turning your Android phone into a pocket Linux server without root? A server thatâ€™s accessible from anywhere in the world and can run your services 24/7 without any cost.\n\nI recently experimented with this idea and would love to share the setup with you. But before diving into the technical steps, let me answer a few questions to build some understanding:\n\n### **What are the core functionalities of a remote Linux server?**\n\n* Accessible from anywhere in the world using any of your devices\n    \n* Able to run services such as web apps, machine learning models, APIs, etc\n    \n* Services keep running even if you log out from your device\n    \n* Can expose services for public visibility or external usage if desired\n    \n* Provides secure access and file management through SSH\n    \n\n### **What youâ€™ll learn from this article?**\n\n* How to turn your Android device into a Linux machine without root\n    \n* How to create a mesh network to make your pocket server accessible outside your local network\n    \n* How to access the remote server from any device using SSH\n    \n\n**In the next part of this article, Iâ€™ll cover:** How to remotely deploy a Flask app on your pocket server, making the app publicly available through free or custom domains and keeping the app running forever using a linux window manager.\n\n### **When to use this Pocket Server?**\n\n* You want a free solution to deploy your fun projects\n    \n* You need a quick and easy setup for rapid prototyping\n    \n* You want your app running and accessible 24/7 without budget limitations\n    \n* Youâ€™re an early-stage learner exploring how a Linux server works\n    \n* You need a personal server for experiments without renting cloud services or hardware\n    \n\n***Note:*** *This setup is not recommended for use as a production server for business applications.*\n\n## Quick Setup Overview\n\n**For experienced developers:** Here's the 30-second summary of what we're building. **If youâ€™re new to this:** The detailed step-by-step guide below covers everything from installation to troubleshooting, with explanations of each concept and command.\n\n**Overview:**\n\n1. **Termux** â†’ Linux environment on Android (no root needed)\n    \n2. **SSH Server** â†’ OpenSSH daemon on port 8022 of Android (Termux)\n    \n3. **Tailscale** â†’ Mesh network for global connectivity (bypasses NAT/firewalls)\n    \n4. **SSH Client** â†’ Remote access from any device on the mesh network\n    \n\n**Architecture:** `Your Computer (SSH Client)` â†â†’ `Tailscale Mesh Network` â†â†’ `Android (Termux + SSH Server)`\n\nNow let's begin with the detailed technical part of this setup. I'll break it down into several steps to help you understand easily.\n\n## Step 1. Enable your android to run linux commands\n\nThe simplest solution is to install the **Termux App** on your android device. You can get it from Google Play Store.\n\n> **What is Termux?** [Termux](https://termux.dev/en/) is a free and open-source terminal emulator and Linux environment for Android devices. It allows users to run a Linux command-line interface (CLI) and access a wide range of Linux tools and utilities directly on their phones or tablets. Essentially, it turns your Android device into a portable Linux machine.\n\n![Termux for Android](https://cdn.hashnode.com/res/hashnode/image/upload/v1756081436890/f6491c7c-8c13-4bba-a318-ba060fcff572.jpeg align=\"center\")\n\n### Install required packages on your Termux\n\n```bash\n# update termux packages\npkg upgrade -y\n\n# install openssh server\npkg install openssh -y\npkg install termux-services -y\n\n# start OpenSSH server\nsshd\n\n# Check if SSH server is running: It shows process ID number\npgrep sshd\n\n# Show username - save this for later! \nwhoami\n# you'll see something like this\nu0_a329 \n```\n\n**Learn more:**\n\n1\\. Termux Remote Access: [https://wiki.termux.com/wiki/Remote\\_Access](https://wiki.termux.com/wiki/Remote_Access)\n\n## Step 2: Create a Mesh Network\n\nTo make your server accessible from anywhere in the world, we'll use **Tailscale** to create a secure mesh network between your devices.\n\n> What is Tailscale? [Tailscale](https://tailscale.com/) is a mesh VPN (Virtual Private Network) service that streamlines connecting devices and services securely across different networks.\n\n### Install Tailscale\n\n**On Android:**\n\n* Install the Tailscale app from Google Play Store\n    \n* Sign up with your email account\n    \n\n**On your computer (Linux/Ubuntu):**\n\n```bash\ncurl -fsSL https://tailscale.com/install.sh | sh\n\n# Start tailscale and connect\nsudo tailscale up\n\n# Show your computer's tailscale ipv4\ntailscale ip -4 \n# you'll see something like: 100.123.173.83\n```\n\nVisit [Tailscale download page](https://tailscale.com/download) to get the tailscale for different OS including android, ios, windows, linux, or mac.\n\n**Important:** Use the same email account on both devices to connect them to the same mesh network.\n\n### Get your Android's Tailscale IP\n\n* Open Tailscale app on Android\n    \n* Note down the IP address shown (e.g., 100.92.201.3)\n    \n* Alternatively, visit your [Tailscale Admin Console](https://login.tailscale.com/admin/machines)\n    \n\n![Tailscale Admin Console (Browser)](https://cdn.hashnode.com/res/hashnode/image/upload/v1756074406266/189e790f-e6bc-4502-9efa-04eaf80c57fe.png align=\"center\")\n\n**Learn more:**\n\n1\\. [Understanding mesh network topology (mesh VPNs)](https://tailscale.com/learn/understanding-mesh-vpns)\n\n## Step 3: Set Up SSH Keys\n\nNow we'll create secure SSH keys to connect to your pocket server.\n\n### Create SSH Key Pair on your computer\n\n```bash\n# Navigate to SSH directory\ncd ~/.ssh\n\n# Generate SSH Keys (replace email with your own)\nssh-keygen -t ed25519 -C taufiqkhantusar@gmail.com\n```\n\nWhen prompted for filename, write a name for your ssh keys. In this example, Iâ€™ve written `pocket_server_key` as the filename.\n\n**Expected output:**\n\n```plaintext\nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/data/data/com.termux/files/home/.ssh/id_ed25519): pocket_server_key\nEnter passphrase for \"pocket_server_key\" (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in pocket_server_key\nYour public key has been saved in pocket_server_key.pub\n```\n\n### Copy the public key\n\n```bash\n# Print and copy the public key\ncat pocket_server_key.pub\n```\n\nCopy the entire output (starts with ssh-ed25519...). For example, my public key is: `ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFIx5eS+ukcJEnqQwpKxvR1C/zn9GAInyvWPqd091vpj` [`taufiqkhantusar@gmail.com`](mailto:taufiqkhantusar@gmail.com)\n\n### Add the public key to your Android (Termux)\n\n```bash\n# Run on Termux\nnano ~/.ssh/authorized_keys \n# Paste the public key, save with Ctrl+S, exit with Ctrl+X\n```\n\nNow, youâ€™ve given access to your computer to connect your pocket server remotely.\n\n## Step 4: Configure Remote Access\n\n### Create SSH config on your computer\n\n```bash\nnano ~/.ssh/config\n```\n\n**Add this configuration (replace with your actual values):**\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1756074102607/1f2d7cdc-3aad-4185-b8f7-aaed913486ae.png align=\"center\")\n\nHere **Hosname** is your Android's Tailscale IP. **User** is your Termux username from `whoami`. **IdentityFile** is the ssh private key you just generated. **Port** is your Termux SSH port.\n\n```bash\nHost my-pocket-server\n    Hostname 100.92.201.3 \n    Port 8022\n    User u0_a329\n    IdentityFile ~/.ssh/pocket_server_key\n    ForwardAgent yes\n    ServerAliveInterval 30\n    ServerAliveCountMax 5\n    TCPKeepAlive yes\n```\n\n> **Why port 8022?** Termux uses port 8022 instead of the standard SSH port 22 for security reasons and to avoid conflicts with system services.\n\n## Step 5: Connect to Your Pocket Server\n\nMake sure SSH is running on Termux using `sshd` on your Android (Termux).\n\nNow SSH from your computer and check that you can access your pocket server remotely.\n\n```bash\n# from your computer\nssh -v my-pocket-server\n```\n\nThe `-v` flag shows verbose output to help troubleshoot any connection issues.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1756073207262/29441402-ba4a-4ad7-8075-af439669a5a9.png align=\"center\")\n\n**ðŸŽ‰ Boom!** You've successfully transformed your Android device into a remote Linux server! Your server is now ready to host applications, run scripts, and serve as your personal development environment remotely!\n\nYou can also SSH from any device on your Tailscale network (just copy your SSH config and keys to other devices).\n\n## Quick Troubleshooting FAQ\n\n**Connection refused?** Check if SSH is running: `pgrep sshd` on Termux\n\n**Permission denied?** Verify your public key is in Termux's `~/.ssh/authorized_keys`\n\n**Can't connect?** Ensure both devices show \"Connected\" in Tailscale Admin Panel\n\n**High battery drain?** This is normal - keep your device plugged in when running services\n\n## What's Next?\n\nIn **Part 2** of this series, we'll take your pocket server to the next level:\n\n* Host a Flask web application\n    \n* Make your app publicly accessible with free and custom domains\n    \n* Set up process management to keep services running 24/7\n    \n* Add monitoring and logging capabilities\n    \n\n**Have questions or ran into issues?** Drop a comment below! I would love helping fellow developers get their pocket servers running.\n\n**Found this helpful?** Share it with your developer friends who love experimenting with creative solutions!"

colab_inference = """As an AI/ML practitioner, you've probably experienced the challenges of GPU access. While platforms like Google Colab and Kaggle offer free GPU support for training and fine-tuning models through notebooks, things get complicated when you need to build AI-powered software. Because not every time you'll code as a Notebook. Fine-tuning a model using free cloud GPU resources is fine, but when it's time to integrate that model into your application for a Proof of Concept (POC) or demo, the limitation of GPU access becomes a real pain point.\n\nThese days, there are numerous paid GPU instances and shared GPU services available. However, choosing the right GPU service can be tricky, even for production environments, when considering factors like uptime, cost, resources, and maintenance. Who wants to invest time and energy researching GPU providers just for a POC or short demo? Additionally, paying for GPUs in this case can be challenging for many learners.\n\nLong story short! Today, we'll explore how to use Google Colab's free 15GB T4 GPU for inference through your local VS Code and Terminal.\n\n**Who should read this article?**\n\n1. AI & SWE practitioners without dedicated GPUs in their personal devices\n    \n2. Those seeking a quick hacks to use their models from anywhere\n    \n3. Application Developers working without a GPU instance budget on development server\n    \n4. Students presenting AI projects\n    \n5. Practitioners needing to show quick demos\n    \n6. Small teams building Proof of Concept (POC) of their SaaS\n    \n\nThis list may grow as we discover more use cases. Now, let's dive in!\n\n# Method & Code\n\nIn this quick tutorial, Iâ€™m using [**EXAONE-3.5-2.4b-instruct**](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct) pretrained model, which requires approximately 10GB disk storage, ~3GB RAM, and ~6GB GPU memory. Feel free to use your own trained/fine-tuned or other models. But remember that Google Colab's Free T4 GPU has a 15GB limit. Our implementation consists of two parts: (1) Loading Model & Creating Inference Endpoint in Google Colab, and (2) HTTP Request to the Inference endpoint from local computer.\n\n## 1\\. Google Colab\n\nAs told earlier, we will run our model on google colabâ€™s GPU and you can inference the model from your local VS Code or Terminal or anywhere. Start by creating a new notebook in [Google Colab](https://colab.research.google.com/) and ensure you switch the runtime to GPU.\n\n## 1.1. Setup Model\n\n```python\n# Install Dependencies\n! pip install torch transformers huggingface_hub structlog\n```\n\n```python\n# Import model dependencies\nimport warnings\nimport structlog\nfrom huggingface_hub import login\nfrom torch import bfloat16\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nlogger = structlog.get_logger(__name__)\n\n# Add Hugging Face Access Token to download the opensource model\nHF_TOKEN = \"Your_HuggingFace_Token\" # https://huggingface.co/settings/tokens\nlogin(HF_TOKEN)\n```\n\n```python\n# Function to download model and tokenizer from hugging face\ndef setup_model(\n    model_name_or_local_path:str=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n    device=\"auto\",\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_local_path)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name_or_local_path,\n        torch_dtype=bfloat16,\n        trust_remote_code=True,\n        device_map=device,\n    )\n    return model, tokenizer\n```\n\n```python\n# Function to inference the model\ndef infer_model(prompt, tokenizer, model, max_tokens=200, device=\"cuda\"):\n    if type(prompt)==str:\n        messages = [\n            {\"role\": \"system\",\n             \"content\": \"You are a helpful chatbot.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    else: messages=prompt\n    logger.info(\"Inference Started\", messages=messages)\n    input_ids = tokenizer.apply_chat_template(\n        messages,\n        tokenize=True,\n        add_generation_prompt=True,\n        return_tensors=\"pt\"\n    )\n    output = model.generate(\n        input_ids.to(device),\n        eos_token_id=tokenizer.eos_token_id,\n        max_new_tokens=max_tokens,\n        do_sample=False,\n    )\n    completion = tokenizer.decode(output[0])\n    # extract the actual generated content from completion. Tweak as per your model's generation\n    content = completion.split(\"[|assistant|]\")[-1].split(\"[|endofturn|]\")[0]\n    logger.info(\"Generation Done\", content=content)\n    return completion, content\n```\n\n```python\n# Setup model and tokenizer. \n# This will basically download the model from HuggingFace and cache the model\n# If you already has the preferred model in disk, pass the model path\nmodel, tokenizer = setup_model(\n    model_name_or_local_path=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n)\n```\n\nPerfect! Your model and tokenizer are now configured. You'll see the model running on Google Colab's GPU as expected.\n\n*\\[Note: The resource consumption shown below is higher because I've been running the notebook for about an hour.\\]*\n\n![Resource usage on Google Colab](https://cdn.hashnode.com/res/hashnode/image/upload/v1736544738564/c7262061-0858-45d3-9344-7b514ef40719.png align=\"center\")\n\n## 1.2. Tunneling\n\nNow it is time to create an publicly accessible endpoint in the same Notebook. So that we can access the model from anywhere and pass prompt & arguments to the model from local.\n\nFirst thing first. Sign in/up to [ngrok.com](https://ngrok.com/) and obtain your Personal Auth-token from [here](https://dashboard.ngrok.com/get-started/your-authtoken). Although this is enough for this tutorial, I recommend getting a static Ngrok domain. After signing in/up to Ngrok, you can create a static domain from [here](https://dashboard.ngrok.com/domains). Do not worry. It is also free!\n\n![Ngrok Free Quota](https://cdn.hashnode.com/res/hashnode/image/upload/v1736547802685/e3101a89-8e69-4602-be38-a213cdb57617.png align=\"center\")\n\n**Why a static Ngrok domain is better?**\n\nIn general, you can proceed without a static domain but each time you run Ngrok tunnel, it generates a new public URL for model access. So, during inference, when you send requests to the inference endpoint, you have to ensure that the public URL is correct. A static domain maintains a same endpoint URL, making inference much more convenient.\n\n**Now, Let's implement the code:**\n\n```python\n# Install ngrok dependencies\n! pip install pyngrok --upgrade\n! pip install flask-ngrok --upgrade\n! pip install flask-cors\n```\n\n```python\n# Import ngrok dependencies\nfrom flask import Flask, request, jsonify\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\n\nNGROK_TOKEN = \"YOUR_NGROK_TOKEN\"\nngrok.kill()\n! ngrok config add-authtoken $NGROK_TOKEN\n```\n\n```python\n# Functions to run NGROK in a Flask Development Server and process Inference request\n\ndef process_inferecne(payload:dict, device=\"cuda\"):\n    prompt = payload.get('prompt')\n    max_tokens = payload.get('max_tokens', 500)\n    logger.info(\"Inference Starting\", prompt=prompt, max_tokens=max_tokens)\n    completion, content = infer_model(prompt, tokenizer, model, max_tokens=max_tokens, device=device)\n    return content\n\ndef run_app():\n    app_port = 5000\n    ngrok_static_domain = \"Your_Static_Domain\" # Your static domain (if any), otherwise comment\n    app = Flask(__name__)\n    CORS(app)\n    public_url = ngrok.connect(\n        addr=app_port, \n        domain=ngrok_static_domain  # Your static domain (if any), otherwise comment\n    )\n    logger.info(f\"NGROK Public URL: {public_url}\")\n\n    @app.route('/gpu-inference', methods=['POST'])\n    def flask_inference():\n        try:\n            payload = request.json\n            logger.info(\"Received inference request\", payload=payload)\n            result = process_inferecne(payload=payload)\n            return jsonify({\"status\": \"success\",\"result\": result}), 200\n        except Exception as e:\n            return jsonify({\"status\": \"error\",\"message\": str(e)}), 500\n    app.run(port=5000)\n```\n\n```python\n# Run your Flask & Tunnel\nrun_app()\n```\n\nBoom! Your inference endpoint is now ready. Now you can access your model running on Google Colab's GPU from anywhere for inference operations. The model is also accessible through your local VS Code or terminal without requiring a dedicated GPU in your computer. Relief right?\n\nNotice the line ***Public URL: NgrokTunnel: \"***[***https://closely-vital-puma.ngrok-free.app***](https://closely-vital-puma.ngrok-free.app)***\" -&gt; \"***[***http://localhost:5000***](http://localhost:5000)***\"*** below. Now, you can send HTTP request to [***<mark>https://closely-vital-puma.ngrok-free.app</mark>***](https://closely-vital-puma.ngrok-free.app)***<mark>/gpu-inference</mark>*** for inference.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1736540412125/bbfa0d16-626a-468f-a4cc-29b34b6b3051.png align=\"center\")\n\nThis concludes the Google Colab setup. Let's move on to running inference from your local environment.\n\n## **2\\. Inference from Local VS Code & Terminal**\n\nWe'll demonstrate how to perform inference using Python3 Interpreter, Ubuntu Terminal, Windows PowerShell, and VS Code one-by-one.\n\n### 2.1. Python Interpreter\n\n```python\ntaufiq_wsl@Taufiq:~$ python3\nPython 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import requests\n>>> headers = {'Content-Type': 'application/json'}\n>>> endpoint = \"https://closely-vital-puma.ngrok-free.app/gpu-inference\"\n>>> prompt = \"Hi Exa! Tell me which filed has highest salary in Bangladesh between AI and Cyber Security? I am specifically asking for Dhaka City.\"\n>>> payload = {\"prompt\":prompt, \"max_tokens\":300}\n>>> response = requests.post( endpoint, json=payload, headers=headers, timeout=30)\n>>> response.json()\n{'result': \"In Dhaka City, Bangladesh, both Artificial Intelligence (AI) and Cyber Security are rapidly growing fields with competitive salaries, but they tend to attract different skill sets and roles, which can influence salary levels based on expertise and experience. Hereâ€™s a general comparison based on current trends:\\n\\n### Cyber Security:\\n1. **High Demand**: Cyber Security is one of the fastest-growing sectors globally, including in Bangladesh.\\n2. **Salary Range**: Salaries in Cyber Security can vary widely depending on experience and specific roles:\\n   - **Entry Level**: BDT 300,000 - BDT 500,000 per annum\\n   - **Mid-Level**: BDT 500,000 - BDT 1,000,000 per annum\\n   - **Senior Level**: BDT 1,000,000+ per annum\\n3. **Key Roles**: Network Security Engineers, Security Analysts, Penetration Testers, Security Architects, Compliance Officers.\\n\\n### Artificial Intelligence (AI):\\n1. **Emerging Field**: AI is still developing in Bangladesh compared to Cyber Security, but itâ€™s gaining traction.\\n2. **Salary Range**: Salaries in AI can be competitive but might be slightly lower due to the field's relative youth and less established infrastructure:\\n   - **\", 'status': 'success'}\n>>>\n>>> # Seems a cutoff in the response. Let's increase token limit and tweak prompt to write concisely\n>>> prompt = \"Hi Exa! Tell me which filed has highest salary in Bangladesh between AI and Cyber Security? I am specifically asking for Dhaka City. Please answer very concisely\"\n>>> payload = {\"prompt\":prompt, \"max_tokens\":500}\n>>> response = requests.post( endpoint, json=payload, headers=headers, timeout=30)\n>>> response.json()\n{'result': \"In Dhaka City, **Cyber Security** generally offers higher salaries compared to AI for similar roles due to increasing demand and criticality in Bangladesh's tech landscape.\", 'status': 'success'}\n>>>\n>>> prompt = \"Hi Exa! Tell me which filed has highest salary in Bangladesh between AI and Cyber Security? I am specifically asking for Dhaka City. Please answer very concisely but include the salary range for junior engineer.\"\n>>> payload = {\"prompt\":prompt, \"max_tokens\":500}\n>>> response = requests.post( endpoint, json=payload, headers=headers, timeout=30)\n>>> response.json()\n{'result': 'In Dhaka City, **Cyber Security** generally offers higher salaries compared to AI for junior engineers. \\n\\n**Salary Range (Junior Engineer):**\\n- **Cyber Security:** BDT 300,000 - BDT 500,000 annually.\\n- **AI:** BDT 250,000 - BDT 400,000 annually.', 'status': 'success'}\n>>>\n```\n\n### 2.2. Ubuntu Terminal (Bash)\n\n```bash\ntaufiq_wsl@Taufiq:~$ curl -X POST \"https://closely-vital-puma.ngrok-free.app/gpu-inference\" -H \"Content-Type: application/json\" -d '{\"prompt\":\"Hi Exa! Tell me which filed has highest salary in Bangladesh between AI and Cyber Security? I am specifically asking for Dhaka City.\",\"max_tokens\":700}' -m 30\n{\"result\":\"In Dhaka City, Bangladesh, both Artificial Intelligence (AI) and Cyber Security are rapidly growing fields with competitive salaries, but they tend to attract different skill sets and roles, which can influence salary levels based on expertise and experience. Here\\u2019s a general comparison based on current trends:\\n\\n### Cyber Security:\\n1. **High Demand**: Cyber Security is one of the fastest-growing sectors globally, including in Bangladesh.\\n2. **Salary Range**: Salaries in Cyber Security can vary widely depending on experience and specific roles:\\n   - **Entry Level**: BDT 300,000 - BDT 500,000 per annum\\n   - **Mid-Level**: BDT 500,000 - BDT 1,000,000 per annum\\n   - **Senior Level**: BDT 1,000,000+ per annum\\n3. **Key Roles**: Network Security Engineers, Security Analysts, Penetration Testers, Security Architects, Compliance Officers.\\n\\n### Artificial Intelligence (AI):\\n1. **Emerging Field**: AI is still developing in Bangladesh compared to Cyber Security, but it\\u2019s gaining traction.\\n2. **Salary Range**: Salaries in AI can be competitive but might be slightly lower due to the field's relative youth and less established infrastructure:\\n   - **Entry Level**: BDT 250,000 - BDT 400,000 per annum\\n   - **Mid-Level**: BDT 400,000 - BDT 800,000 per annum\\n   - **Senior Level**: BDT 800,000+ per annum\\n3. **Key Roles**: Machine Learning Engineers, Data Scientists, AI Researchers, AI Product Managers, AI Ethicists.\\n\\n### Conclusion:\\nWhile Cyber Security tends to offer slightly higher salaries across most levels due to its established presence and critical importance in protecting digital assets, both fields are lucrative. **Cyber Security** might edge out slightly due to its broader applicability and higher demand for specialized skills in Dhaka City. However, **AI** is rapidly expanding and could offer significant opportunities for high earners as the technology matures further in Bangladesh.\\n\\nFor precise figures, it would be beneficial to consult recent job listings, salary surveys specific to Dhaka City, or professional networking platforms like LinkedIn to get the most current and detailed insights.\",\"status\":\"success\"}\ntaufiq_wsl@Taufiq:~$\n```\n\n### 2.3. Windows PowerShell\n\n```bash\nPowerShell 7.4.6\nPS C:\\Users\\DELL> curl.exe -X POST \"https://closely-vital-puma.ngrok-free.app/gpu-inference\" -H \"Content-Type: application/json\" -d '{\"prompt\":\"Hi Exa! Tell me which filed has highest salary in Bangladesh between AI and Cyber Security? I am specifically asking for Dhaka City.\",\"max_tokens\":700}' -m 30\n{\"result\":\"In Dhaka City, Bangladesh, both Artificial Intelligence (AI) and Cyber Security are rapidly growing fields with competitive salaries, but they tend to attract different skill sets and roles, which can influence salary levels based on expertise and experience. Here\\u2019s a general comparison based on current trends:\\n\\n### Cyber Security:\\n1. **High Demand**: Cyber Security is one of the fastest-growing sectors globally, including in Bangladesh.\\n2. **Salary Range**: Salaries in Cyber Security can vary widely depending on experience and specific roles:\\n   - **Entry Level**: BDT 300,000 - BDT 500,000 per annum\\n   - **Mid-Level**: BDT 500,000 - BDT 1,000,000 per annum\\n   - **Senior Level**: BDT 1,000,000+ per annum\\n3. **Key Roles**: Network Security Engineers, Security Analysts, Penetration Testers, Security Architects, Compliance Officers.\\n\\n### Artificial Intelligence (AI):\\n1. **Emerging Field**: AI is still developing in Bangladesh compared to Cyber Security, but it\\u2019s gaining traction.\\n2. **Salary Range**: Salaries in AI can be competitive but might be slightly lower due to the field's relative youth and less established infrastructure:\\n   - **Entry Level**: BDT 250,000 - BDT 400,000 per annum\\n   - **Mid-Level**: BDT 400,000 - BDT 800,000 per annum\\n   - **Senior Level**: BDT 800,000+ per annum\\n3. **Key Roles**: Machine Learning Engineers, Data Scientists, AI Researchers, AI Product Managers, AI Ethicists.\\n\\n### Conclusion:\\nWhile Cyber Security tends to offer slightly higher salaries across most levels due to its established presence and critical importance in protecting digital assets, both fields are lucrative. **Cyber Security** might edge out slightly due to its broader applicability and higher demand for specialized skills in Dhaka City. However, **AI** is rapidly expanding and could offer significant opportunities for high earners as the technology matures further in Bangladesh.\\n\\nFor precise figures, it would be beneficial to consult recent job listings, salary surveys specific to Dhaka City, or professional networking platforms like LinkedIn to get the most current and detailed insights.\",\"status\":\"success\"}\nPS C:\\Users\\DELL>\n```\n\n### 2.4. VS Code\n\n```python\n# Write in a '.py' file\nimport requests\n\ndef inference_from_vscode(prompt, max_tokens=700, timeout=30):\n    headers = {\"Content-Type\": \"application/json\"}\n    endpoint = \"https://closely-vital-puma.ngrok-free.app/gpu-inference\"\n    payload = {\"prompt\": prompt, \"max_tokens\": max_tokens}\n    response = requests.post(endpoint, json=payload, headers=headers, timeout=timeout)\n    return response.json()\n\nif __name__ == \"__main__\":\n    prompt = \"Hi Exa! Tell me which filed has highest salary in Bangladesh between AI and Cyber Security?\"\n    response = inference_from_vscode(prompt)\n    print(response[\"result\"])\n```\n\nRun the above code as a python file and youâ€™ll get output like below.\n\n**Output:**\n\n```markdown\nIn Bangladesh, both Artificial Intelligence (AI) and Cyber Security are rapidly growing fields with significant demand, but the salary structures can vary based on several factors including experience level, specific roles, company size, and location within the country. Hereâ€™s a general comparison based on recent trends:\n\n### Cyber Security:\n1. **Entry-Level Positions**: Typically, entry-level positions like Junior Security Analyst or Security Engineer can start around BDT 300,000 to BDT 500,000 per annum.\n2. **Mid-Level Positions**: Mid-level roles such as Security Architect or Senior Security Analyst might range from BDT 500,000 to BDT 1,000,000 annually.\n3. **Senior Positions**: Senior roles like Chief Information Security Officer (CISO) or Director of Security can command salaries upwards to BDT 1,500,000 to BDT 3,000,000 or more annually.\n\n### Artificial Intelligence (AI):\n1. **Entry-Level Positions**: For AI roles like Machine Learning Engineer or Data Scientist, entry-level salaries can start around BDT 300,000 to BDT 600,000 per annum.\n2. **Mid-Level Positions**: Mid-level roles such as Senior Machine Learning Engineer or AI Research Scientist might range from BDT 600,000 to BDT 1,200,000 annually.\n3. **Senior Positions**: Senior positions like Chief AI Officer or Director of AI Research can earn significantly more, often ranging from BDT 1,200,000 to BDT 3,000,000 or more annually.\n\n### Summary:\n- **Cyber Security** tends to have a strong presence in foundational roles with competitive salaries, especially as the field matures and demand increases.\n- **AI** is growing rapidly, particularly in research and development roles, which can offer higher salaries due to the specialized nature of the work and the increasing importance of AI technologies across various industries.\n\n### Factors Influencing Salary:\n- **Experience**: More experienced professionals typically earn higher salaries.\n- **Company Size and Industry**: Larger companies and those in sectors heavily reliant on technology (like finance, healthcare, and tech startups) often offer higher salaries.\n- **Skill Set**: Specialized skills in AI, particularly in areas like deep learning, natural language processing, etc., can command premium salaries.\n\nGiven these points, **AI** roles, especially at senior levels, often have the potential to offer higher salaries due to the specialized nature of the work and the broader applicability of AI technologies across multiple industries. However, the exact figures can vary widely based on individual circumstances and market conditions.\n```\n\n# Conclusion\n\nIn this tutorial, we've explored how to leverage Google Colab's free GPU for inference from local devices. While some cloud providers offer limited free GPU access for application development (Flask, Django, etc.), in most cases they require educational or organizational email addresses. Instead, we've focused on a solution accessible to everyone\n\nFeel free to check out the complete [Google Colab Notebook](https://colab.research.google.com/drive/1ySnuNhi69k84EAI3nlmj4RlHybBw1pAx?usp=sharing) for this tutorial. The code is available for you to copy and adapt for your use cases. If youâ€™re curious to learn a bit more about the **EXAONE-3.5** model, please refer to this article titled â€œ[EXAONE-3.5-2.4B: A Ultra-lightweight but High Performing LLM on Just 6GB GPU](https://taufiq.hashnode.dev/exaone-3-5-2-4b-pre-trained)â€œ. Good luck with your AI Journey. Thank you!\n\n***About the Author:*** *Taufiq is an Artificial Intelligence Engineer from Bangladesh, currently working remotely at* [***LaLoka Labs, Tokyo, Japan***](https://lalokalabs.co/en/) *as an NLP Engineer & Backend Developer (Level 3). Feel free to connect with him on LinkedIn:* [*https://www.linkedin.com/in/taufiq-khan-tusar/*](https://www.linkedin.com/in/taufiq-khan-tusar/)*.*"""

exaone = """<p>Today, there are many Large Language Models (LLMs) available, ranging from billions to trillions of parameters in size. While the list of LLMs is extensive, it becomes much shorter when considering in-house models with lower GPU requirements. Performance remains the critical factor, and finding a model that combines low GPU consumption with high performance is increasingly rare in today's landscape.</p>\n<p>Recently, I explored the <a target="_blank" href="https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"><strong>EXAONE-3.5-2.4B-Instruct</strong></a> model that was developed by <a target="_blank" href="https://www.lgresearch.ai/">LG AI Research</a>. I specifically tested the model for various tasks including NER, Text Classification, Code Generation, Q&amp;A, Summarization, Text Generation, and Article Generation. The results showed better performance compared to similar-sized models such as IBM's Granite 2B, Google's Gemma 2B, and Microsoft's Phi-2. Additionally, EXAONE-3.5-2.4B-Instruct consumes only ~6GB of GPU memory during inference using the pre-trained model.</p>\n<h2 id="heading-background">Background</h2>\n<p>Before diving into the details, let me share what led me to discover this model. I'm working on a RAG chatbot project that handles sensitive business data and requires high contextual understanding. The project faces several key challenges:</p>\n<ol>\n<li><p><strong>Data Security:</strong> Sensitive business data that should not be exposed at any case.</p>\n</li>\n<li><p><strong>Context Window:</strong> Token count grows exponentially with chat history, making commercial LLM APIs like OpenAI and Anthropic prohibitively expensive.</p>\n</li>\n<li><p><strong>Multi-Task Handling:</strong> The system needs to handle diverse tasks including NER, Python &amp; SQL code generation, moderation, segmentation, and RAG-based Q&amp;A. This requires either a versatile single model or multiple specialized models.</p>\n</li>\n<li><p><strong>Latency:</strong> One task waits on another task's output, so latency should be minimal. Although the GPT 4o Mini, Claude 3.5 Haiku offer lower latency, they sometimes face overload issues.</p>\n</li>\n<li><p><strong>Limited Budget:</strong> GPU requirements should stay around 5GB for inference to remain cost-effective.</p>\n</li>\n<li><p><strong>Accuracy:</strong> High contextual understanding and accuracy is essential for reliable performance.</p>\n</li>\n</ol>\n<p>After evaluating these requirements, I explored several top-ranked smaller models from the Open LLM Leaderboard, such as <strong>tiiuae/Falcon3-1B-Instruct</strong>, <strong>google/gemma-2b</strong>, <strong>ibm-granite/granite-3.1-2b-instruct</strong>, <strong>microsoft/phi-2</strong>. EXAONE-3.5-2.4B-Instruct stood out for its strong contextual understanding, multi-task capabilities, efficient resource usage (only ~6GB GPU &amp; 2GB RAM), and structured output formatting.</p>\n<h2 id="heading-benchmark">Benchmark</h2>\n<p><strong>EXAONE-3.5</strong> comes in three variants: 2.4B, 7.8B, and 32B parameters. Below are benchmark comparison scores of the 2.4B &amp; 7.8B variants against similar models.</p>\n<p><img src="https://media.licdn.com/dms/image/v2/D5612AQF-KP0j08w1aQ/article-inline_image-shrink_400_744/article-inline_image-shrink_400_744/0/1735449260094?e=1741219200&amp;v=beta&amp;t=m9buwZYML2nnnetVtzjqV_m_2mtvTTPsvCJIZ7Gqhdw" alt="Benchmark score comparison" class="image--center mx-auto" /></p>\n<h2 id="heading-hardware-requirements">Hardware Requirements</h2>\n<p>Testing was conducted using the Kaggle Free Tier with the following specifications:</p>\n<ul>\n<li><p>GPU: Tesla P100 16GB</p>\n</li>\n<li><p>RAM: 29GB</p>\n</li>\n<li><p>Disk: 60GB</p>\n</li>\n</ul>\n<p>After a 2-hour testing session, the resource consumption was notably efficient:</p>\n<ol>\n<li><p>GPU Memory: 6.3GB</p>\n</li>\n<li><p>RAM Usage: ~2GB</p>\n</li>\n<li><p>Disk Space: 11.1GB</p>\n</li>\n</ol>\n<h2 id="heading-code">Code</h2>\n<p>Let's go through the <a target="_blank" href="https://www.kaggle.com/code/taufiqtusar/exaone-3-5-2-4b-instruct-pre-trained-model-test">Kaggle Notebook</a> to explore the pretrained variant of EXAONE-3.5-2.4B-Instruct. The notebook is ready to run on the go - just do not forget to change the accelerator to GPU. It'll take approx. 3-4 minutes to setup the model and tokenizer. For detailed implementation guidelines, you can refer to the model's documentation <a target="_blank" href="https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct">here</a>.</p>\n<p>I've explored the following use cases in this notebook:</p>\n<ol>\n<li><p>Named Entity Recognition (NER)</p>\n</li>\n<li><p>Text Classification</p>\n</li>\n<li><p>Python &amp; HTML Code Generation</p>\n</li>\n<li><p>Q&amp;A</p>\n</li>\n<li><p>Text Summarization</p>\n</li>\n<li><p>Text Generation</p>\n</li>\n</ol>\n<h2 id="heading-future-works">Future Works</h2>\n<p>Next, I plan to explore the following with <strong>EXAONE-3.5-2.4B</strong>:</p>\n<ul>\n<li><p><strong>Fine-tuning:</strong> In this exploration, I used the pre-trained model for quick testing. For better output, fine-tuning is necessary. I believe the JSON formatting and context understanding will improve after fine-tuning.</p>\n</li>\n<li><p><strong>Quantization:</strong> This will optimize GPU consumption while maintaining the model's current knowledge in the fine-tuned version.</p>\n</li>\n<li><p><strong>Multilingual Capability:</strong> The EXAONE model currently performs well only in English and Korean, but I require multilingual capability. Further exploration is needed in this area.</p>\n</li>\n</ul>\n<h2 id="heading-conclusion">Conclusion</h2>\n<p>EXAONE-3.5-2.4B-Instruct demonstrates that smaller language models can deliver effective performance while being resource-efficient. With its modest ~6GB GPU requirement and strong capabilities across multiple NLP tasks, it serves as an excellent choice for production deployments where both performance and resource optimization are crucial.</p>\n<p>You can find the documentation and implementation details in the notebook. Feel free to use the code for your use cases. Thank you.</p>\n"""